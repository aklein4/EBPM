debug: false


project: EBPM
name: default

notes: null


batch_size: 256


model:

    type: ebpm.modelling_ebpm.EBPMModel
    pretrained: null

    config:

        type: ebpm.configuration_ebpm.EBPMConfig
        
        kwargs:

            vocab_size: 27
            pad_token_id: 0
            bos_token_id: null
            eos_token_id: null

            hidden_size: 1024
            intermediate_size: 2560

            num_attention_heads: 16
            num_key_value_head: 4

            num_hidden_layers: 24

            rms_norm_eps: 1.0e-06

            initializer_range: null
           
            use_cache: false
            tie_word_embeddings: false

            first_ebm_layer: 12

            lora_rank: 256
            lora_init_scale: 0.1

            ebm_head_init_scale: 0.01

            _attn_implementation: flash_attention_2


trainer:

    type: ebpm_trainer.EBPMTrainer

    skip_steps: null

    gradient_checkpointing: true
    autocast_dtype: bfloat16
    grad_norm_clip: 1.0 

    checkpoint_interval: 1000
    manual_checkpoint_interval: 10
    push_to_hub: true


optimizer:

    type: adamw.AdamW

    kwargs:

        lr: 2e-4
        final_lr: 2e-5

        num_warmup_steps: 500
        num_training_steps: 1000000

        betas: [0.9, 0.95]
        eps: 1e-8

        weight_decay: 0.1

        grad_clip: null

        nan_safe: false


dataset:

    name: aklein4/esm2_uniref_pretraining_data-512
    
    kwargs:

        split: train

        streaming: false


collator:

    type: amino_acid.AminoAcidCollator

    kwargs: {}
